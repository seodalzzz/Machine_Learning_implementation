{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seodalzzz/Machine_Learning_implementation/blob/main/one_layer_logistic_regression_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c098d5d0",
      "metadata": {
        "id": "c098d5d0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d2fe3e",
      "metadata": {
        "id": "a7d2fe3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44015214-f02c-49d7-f77c-737de86f8c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 102432748.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 41944492.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28974608.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 13725164.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "# 1*28*28 -> 784 = 28*28\n",
        "train_data = MNIST(root=path,train=True,transform=transform,download=True)\n",
        "test_data = MNIST(root=path,train=False,transform=transform,download=True)\n",
        "\n",
        "# choose train data with label 0 or 1\n",
        "idx = (train_data.targets==0) | (train_data.targets==1)\n",
        "train_data.targets = train_data.targets[idx]\n",
        "train_data.data = train_data.data[idx]\n",
        "\n",
        "# choose test data with label 0 or 1\n",
        "idx = (test_data.targets==0) | (test_data.targets==1)\n",
        "test_data.targets = test_data.targets[idx]\n",
        "test_data.data = test_data.data[idx]\n",
        "\n",
        "batch_size = 85\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data,batch_size=len(test_data),shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11002ec5",
      "metadata": {
        "id": "11002ec5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1*28*28 -> 784\n",
        "w = np.random.randn(784,1)\n",
        "b = np.random.randn(1,1)\n",
        "#learning rate\n",
        "eta = 1e-4\n",
        "delta = 1e-10 # prevent log 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4b0ade",
      "metadata": {
        "id": "ca4b0ade"
      },
      "outputs": [],
      "source": [
        "# define sigmoid function\n",
        "def sigmoid(val):\n",
        "    result=1/(1+np.exp(-val))\n",
        "    return result\n",
        "\n",
        "# define derivative of sigmoid function w.r.t. its value\n",
        "def grad_sigmoid(val):\n",
        "    result=sigmoid(val)*(1-sigmoid(val))\n",
        "    return result\n",
        "\n",
        "# given data instances in batch form\n",
        "# compute loss and gradients of w and b\n",
        "# count the number of correct prediction\n",
        "def compute_loss_and_grad(data_instance,params):\n",
        "    x, y = data_instance #x.shape=(batch_size,784), w.shape=(784,1)\n",
        "\n",
        "    w, b= params\n",
        "    b=np.tile(b,(x.shape[0],1))\n",
        "    linear=np.dot(w.T,x.T).T+b\n",
        "    p=sigmoid(linear)\n",
        "    loss=-y*np.log(p+delta)-(1-y)*np.log(1-p+delta)\n",
        "    grad= -y*(1-p)+(1-y)*p\n",
        "    grad_w=np.multiply(grad,x)\n",
        "    grad_b=grad\n",
        "\n",
        "    hit=sum(y==np.round(p))\n",
        "    return loss, (grad_w, grad_b), hit\n",
        "\n",
        "# update NN parameters w and b with SGD\n",
        "def update_parameters(params,grads):\n",
        "    w, b = params\n",
        "    grad_w, grad_b = grads\n",
        "\n",
        "    w-=eta*np.mean(grad_w,axis=0).reshape(-1,1)\n",
        "    b-=eta*np.mean(grad_b,axis=0).reshape(-1,1)\n",
        "\n",
        "    return w, b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9591e77c",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9591e77c",
        "outputId": "e898a3c5-78d0-4089-a20f-453de9699a8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train: 3.472 / 38.06 %\n",
            "Epoch 0 Test: 3.575 / 36.78 %\n",
            "\n",
            "Epoch 10 Train: 2.818 / 41.45 %\n",
            "Epoch 10 Test: 2.885 / 40.05 %\n",
            "\n",
            "Epoch 20 Train: 2.268 / 46.31 %\n",
            "Epoch 20 Test: 2.305 / 45.58 %\n",
            "\n",
            "Epoch 30 Train: 1.836 / 52.35 %\n",
            "Epoch 30 Test: 1.848 / 50.97 %\n",
            "\n",
            "Epoch 40 Train: 1.511 / 58.37 %\n",
            "Epoch 40 Test: 1.502 / 55.89 %\n",
            "\n",
            "Epoch 50 Train: 1.267 / 63.77 %\n",
            "Epoch 50 Test: 1.242 / 61.47 %\n",
            "\n",
            "Epoch 60 Train: 1.082 / 68.21 %\n",
            "Epoch 60 Test: 1.045 / 66.38 %\n",
            "\n",
            "Epoch 70 Train: 0.940 / 72.15 %\n",
            "Epoch 70 Test: 0.892 / 70.83 %\n",
            "\n",
            "Epoch 80 Train: 0.827 / 75.10 %\n",
            "Epoch 80 Test: 0.773 / 74.00 %\n",
            "\n",
            "Epoch 90 Train: 0.737 / 77.87 %\n",
            "Epoch 90 Test: 0.679 / 77.45 %\n",
            "\n",
            "Epoch 100 Train: 0.664 / 79.96 %\n",
            "Epoch 100 Test: 0.602 / 80.00 %\n",
            "\n",
            "Epoch 110 Train: 0.603 / 81.71 %\n",
            "Epoch 110 Test: 0.540 / 81.99 %\n",
            "\n",
            "Epoch 120 Train: 0.553 / 83.17 %\n",
            "Epoch 120 Test: 0.488 / 83.26 %\n",
            "\n",
            "Epoch 130 Train: 0.510 / 84.54 %\n",
            "Epoch 130 Test: 0.444 / 84.96 %\n",
            "\n",
            "Epoch 140 Train: 0.474 / 85.77 %\n",
            "Epoch 140 Test: 0.408 / 86.19 %\n",
            "\n",
            "Epoch 150 Train: 0.443 / 86.89 %\n",
            "Epoch 150 Test: 0.377 / 87.52 %\n",
            "\n",
            "Epoch 160 Train: 0.416 / 87.83 %\n",
            "Epoch 160 Test: 0.350 / 88.79 %\n",
            "\n",
            "Epoch 170 Train: 0.392 / 88.76 %\n",
            "Epoch 170 Test: 0.327 / 89.69 %\n",
            "\n",
            "Epoch 180 Train: 0.371 / 89.36 %\n",
            "Epoch 180 Test: 0.306 / 90.64 %\n",
            "\n",
            "Epoch 190 Train: 0.353 / 90.01 %\n",
            "Epoch 190 Test: 0.288 / 91.25 %\n",
            "\n",
            "Epoch 200 Train: 0.336 / 90.58 %\n",
            "Epoch 200 Test: 0.273 / 91.54 %\n",
            "\n",
            "Epoch 210 Train: 0.321 / 91.12 %\n",
            "Epoch 210 Test: 0.259 / 91.87 %\n",
            "\n",
            "Epoch 220 Train: 0.308 / 91.52 %\n",
            "Epoch 220 Test: 0.246 / 92.29 %\n",
            "\n",
            "Epoch 230 Train: 0.295 / 91.94 %\n",
            "Epoch 230 Test: 0.235 / 92.62 %\n",
            "\n",
            "Epoch 240 Train: 0.284 / 92.21 %\n",
            "Epoch 240 Test: 0.224 / 93.05 %\n",
            "\n",
            "Epoch 250 Train: 0.274 / 92.47 %\n",
            "Epoch 250 Test: 0.215 / 93.38 %\n",
            "\n",
            "Epoch 260 Train: 0.265 / 92.85 %\n",
            "Epoch 260 Test: 0.206 / 93.71 %\n",
            "\n",
            "Epoch 270 Train: 0.256 / 93.08 %\n",
            "Epoch 270 Test: 0.199 / 93.95 %\n",
            "\n",
            "Epoch 280 Train: 0.248 / 93.31 %\n",
            "Epoch 280 Test: 0.191 / 94.28 %\n",
            "\n",
            "Epoch 290 Train: 0.241 / 93.53 %\n",
            "Epoch 290 Test: 0.185 / 94.66 %\n",
            "\n",
            "Epoch 300 Train: 0.234 / 93.71 %\n",
            "Epoch 300 Test: 0.179 / 94.80 %\n",
            "\n",
            "Epoch 310 Train: 0.227 / 93.92 %\n",
            "Epoch 310 Test: 0.173 / 94.94 %\n",
            "\n",
            "Epoch 320 Train: 0.221 / 94.15 %\n",
            "Epoch 320 Test: 0.168 / 95.13 %\n",
            "\n",
            "Epoch 330 Train: 0.215 / 94.32 %\n",
            "Epoch 330 Test: 0.163 / 95.27 %\n",
            "\n",
            "Epoch 340 Train: 0.210 / 94.51 %\n",
            "Epoch 340 Test: 0.158 / 95.27 %\n",
            "\n",
            "Epoch 350 Train: 0.205 / 94.59 %\n",
            "Epoch 350 Test: 0.154 / 95.32 %\n",
            "\n",
            "Epoch 360 Train: 0.200 / 94.76 %\n",
            "Epoch 360 Test: 0.150 / 95.60 %\n",
            "\n",
            "Epoch 370 Train: 0.196 / 94.88 %\n",
            "Epoch 370 Test: 0.146 / 95.74 %\n",
            "\n",
            "Epoch 380 Train: 0.192 / 94.96 %\n",
            "Epoch 380 Test: 0.142 / 96.03 %\n",
            "\n",
            "Epoch 390 Train: 0.187 / 95.11 %\n",
            "Epoch 390 Test: 0.139 / 96.22 %\n",
            "\n",
            "Epoch 400 Train: 0.184 / 95.22 %\n",
            "Epoch 400 Test: 0.135 / 96.26 %\n",
            "\n",
            "Epoch 410 Train: 0.180 / 95.32 %\n",
            "Epoch 410 Test: 0.132 / 96.41 %\n",
            "\n",
            "Epoch 420 Train: 0.177 / 95.40 %\n",
            "Epoch 420 Test: 0.129 / 96.45 %\n",
            "\n",
            "Epoch 430 Train: 0.173 / 95.55 %\n",
            "Epoch 430 Test: 0.127 / 96.50 %\n",
            "\n",
            "Epoch 440 Train: 0.170 / 95.65 %\n",
            "Epoch 440 Test: 0.124 / 96.60 %\n",
            "\n",
            "Epoch 450 Train: 0.167 / 95.74 %\n",
            "Epoch 450 Test: 0.122 / 96.69 %\n",
            "\n",
            "Epoch 460 Train: 0.164 / 95.89 %\n",
            "Epoch 460 Test: 0.119 / 96.74 %\n",
            "\n",
            "Epoch 470 Train: 0.162 / 95.95 %\n",
            "Epoch 470 Test: 0.117 / 96.78 %\n",
            "\n",
            "Epoch 480 Train: 0.159 / 96.04 %\n",
            "Epoch 480 Test: 0.115 / 96.88 %\n",
            "\n",
            "Epoch 490 Train: 0.157 / 96.12 %\n",
            "Epoch 490 Test: 0.113 / 97.02 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 500\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "    # train the logistic regression model\n",
        "    total_loss_train = 0\n",
        "    count = 0\n",
        "    for _, (x, y) in enumerate(train_loader):\n",
        "        # 85*1*28*28 -> -1=85,784\n",
        "        x, y = x.numpy().reshape(-1,784), y.numpy().reshape(-1,1) #x:(batch_size,1,28,28)->(batch_size,784)\n",
        "        params = (w, b) #w,b:(784,1)\n",
        "        # compute loss and gradients ->update the parameters\n",
        "        # compute sum of the loss and the number of correct prediction in the batch\n",
        "        loss, (grad_w, grad_b), hit=compute_loss_and_grad((x,y),params)\n",
        "        grads=(grad_w,grad_b)\n",
        "        w,b=update_parameters(params,grads)\n",
        "        total_loss_train+=loss.sum()\n",
        "        count+=hit.sum()\n",
        "\n",
        "    # compute average loss and accuracy for the train dataset\n",
        "    loss_train = total_loss_train/len(train_data)\n",
        "    acc_train = count/len(train_data)\n",
        "\n",
        "    # test, or evaluate, the trained logistic regression model\n",
        "    dataiter = iter(test_loader)\n",
        "    te_images, te_labels = next(iter(dataiter))\n",
        "    total_loss_test = 0\n",
        "    count_test = 0\n",
        "\n",
        "    te_images=te_images.numpy().reshape(-1,784)\n",
        "    te_labels=te_labels.numpy().reshape(-1,1)\n",
        "    # compute loss\n",
        "    # compute sum of the loss and the number of correct prediction\n",
        "    loss, (grad_w, grad_b), hit = compute_loss_and_grad((te_images,te_labels),(w,b))\n",
        "    total_loss_test+=loss.sum()\n",
        "\n",
        "    # compute average loss and accuracy for the test dataset\n",
        "\n",
        "    loss_test = total_loss_test/len(te_labels)\n",
        "    acc_test = hit.sum()/len(te_labels)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(\"Epoch %d Train: %.3f / %.2f %%\"%(i,loss_train,acc_train*100))\n",
        "        print(\"Epoch %d Test: %.3f / %.2f %%\"%(i,loss_test,acc_test*100))\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3272e37",
      "metadata": {
        "id": "f3272e37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae039a2-07ab-4ad0-a567-eae5ae83d2e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss : 0.1109570175199161\n",
            "accuracy : 97.06855791962174\n"
          ]
        }
      ],
      "source": [
        "# final loss / accuracy\n",
        "print(\"loss :\", loss_test)\n",
        "print(\"accuracy :\",acc_test*100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
